{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0hEaPmaAja5Btz5yaYccP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","import pickle\n","import os\n","\n","# Configuration\n","MAX_FEATURES = 10000  # vocabulary size\n","MAX_LEN = 500\n","EMBEDDING_DIM = 128\n","RNN_UNITS = 128\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","VALIDATION_SPLIT = 0.2\n","\n","def load_and_preprocess_data():\n","    \"\"\"Load and preprocess IMDB dataset.\"\"\"\n","    print(\"Loading IMDB dataset...\")\n","    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=MAX_FEATURES)\n","\n","    print(f'Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}')\n","    print(f'Testing data shape: {X_test.shape}, Testing labels shape: {y_test.shape}')\n","\n","    # Pad sequences\n","    print(f\"Padding sequences to max length: {MAX_LEN}\")\n","    X_train = sequence.pad_sequences(X_train, maxlen=MAX_LEN)\n","    X_test = sequence.pad_sequences(X_test, maxlen=MAX_LEN)\n","\n","    return (X_train, y_train), (X_test, y_test)\n","\n","def create_model():\n","    \"\"\"Create and compile the RNN model.\"\"\"\n","    print(\"Creating model architecture...\")\n","    model = Sequential([\n","        Embedding(MAX_FEATURES, EMBEDDING_DIM, input_length=MAX_LEN, name='embedding'),\n","        SimpleRNN(RNN_UNITS, activation='relu', name='rnn'),\n","        Dense(1, activation='sigmoid', name='output')\n","    ])\n","\n","    model.compile(\n","        optimizer='adam',\n","        loss='binary_crossentropy',\n","        metrics=['accuracy']\n","    )\n","\n","    return model\n","\n","def train_model(model, X_train, y_train):\n","    \"\"\"Train the model with early stopping.\"\"\"\n","    print(\"Training model...\")\n","\n","    # Create early stopping callback\n","    early_stop = EarlyStopping(\n","        monitor='val_loss',\n","        patience=5,\n","        restore_best_weights=True,\n","        verbose=1\n","    )\n","\n","    # Train the model\n","    history = model.fit(\n","        X_train, y_train,\n","        epochs=EPOCHS,\n","        batch_size=BATCH_SIZE,\n","        validation_split=VALIDATION_SPLIT,\n","        callbacks=[early_stop],\n","        verbose=1\n","    )\n","\n","    return history\n","\n","def save_model_and_artifacts(model):\n","    \"\"\"Save model and required artifacts for deployment.\"\"\"\n","\n","    # Create directory for model artifacts\n","    os.makedirs('model_artifacts', exist_ok=True)\n","\n","    # 1. Save model in Keras 3 compatible format\n","    print(\"Saving model in .keras format...\")\n","    model.save('model_artifacts/sentiment_model.keras')\n","\n","    # 2. Also save as H5 for backward compatibility\n","    print(\"Saving model in .h5 format...\")\n","    model.save('model_artifacts/sentiment_model.h5')\n","\n","    # 3. Save word index and reverse word index\n","    print(\"Saving word indices...\")\n","    word_index = imdb.get_word_index()\n","    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","    with open('model_artifacts/word_index.pkl', 'wb') as f:\n","        pickle.dump(word_index, f)\n","\n","    with open('model_artifacts/reverse_word_index.pkl', 'wb') as f:\n","        pickle.dump(reverse_word_index, f)\n","\n","    # 4. Save model configuration\n","    config = {\n","        'max_features': MAX_FEATURES,\n","        'max_len': MAX_LEN,\n","        'embedding_dim': EMBEDDING_DIM,\n","        'rnn_units': RNN_UNITS\n","    }\n","\n","    with open('model_artifacts/model_config.pkl', 'wb') as f:\n","        pickle.dump(config, f)\n","\n","    print(\"All artifacts saved successfully!\")\n","\n","def evaluate_model(model, X_test, y_test):\n","    \"\"\"Evaluate model performance.\"\"\"\n","    print(\"Evaluating model...\")\n","    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","    print(f\"Test Loss: {test_loss:.4f}\")\n","    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","    return test_loss, test_accuracy\n","\n","def main():\n","    \"\"\"Main training pipeline.\"\"\"\n","    print(\"Starting IMDB Sentiment Analysis Training Pipeline\")\n","    print(\"=\" * 50)\n","\n","    # Load and preprocess data\n","    (X_train, y_train), (X_test, y_test) = load_and_preprocess_data()\n","\n","    # Create model\n","    model = create_model()\n","    model.summary()\n","\n","    # Train model\n","    history = train_model(model, X_train, y_train)\n","\n","    # Evaluate model\n","    evaluate_model(model, X_test, y_test)\n","\n","    # Save model and artifacts\n","    save_model_and_artifacts(model)\n","\n","    print(\"\\nTraining completed successfully!\")\n","    print(\"Model artifacts saved in 'model_artifacts' directory\")\n","\n","    return model, history\n","\n","if __name__ == \"__main__\":\n","    model, history = main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":935},"id":"2W5Tdz0QprB3","executionInfo":{"status":"ok","timestamp":1756318077549,"user_tz":-60,"elapsed":30266,"user":{"displayName":"Thomas Adegoke","userId":"17281489456952723144"}},"outputId":"2315803f-5494-4bbc-fc6f-06c0d1d0dae7"},"execution_count":2,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Starting IMDB Sentiment Analysis Training Pipeline\n","==================================================\n","Loading IMDB dataset...\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Training data shape: (25000,), Training labels shape: (25000,)\n","Testing data shape: (25000,), Testing labels shape: (25000,)\n","Padding sequences to max length: 500\n","Creating model architecture...\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ output (\u001b[38;5;33mDense\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training model...\n","Epoch 1/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 209ms/step - accuracy: 0.5911 - loss: 240944431104.0000 - val_accuracy: 0.7004 - val_loss: 0.5821\n","Epoch 2/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 203ms/step - accuracy: 0.6869 - loss: 0.8139 - val_accuracy: 0.7254 - val_loss: 0.5477\n","Epoch 3/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 208ms/step - accuracy: 0.5583 - loss: nan - val_accuracy: 0.5062 - val_loss: nan\n","Epoch 4/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 201ms/step - accuracy: 0.5011 - loss: nan - val_accuracy: 0.5062 - val_loss: nan\n","Epoch 5/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 208ms/step - accuracy: 0.5020 - loss: nan - val_accuracy: 0.5062 - val_loss: nan\n","Epoch 6/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 205ms/step - accuracy: 0.5025 - loss: nan - val_accuracy: 0.5062 - val_loss: nan\n","Epoch 7/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 208ms/step - accuracy: 0.5005 - loss: nan - val_accuracy: 0.5062 - val_loss: nan\n","Epoch 7: early stopping\n","Restoring model weights from the end of the best epoch: 2.\n","Evaluating model...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.5435\n","Test Accuracy: 0.7271\n","Saving model in .keras format...\n","Saving model in .h5 format...\n","Saving word indices...\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","All artifacts saved successfully!\n","\n","Training completed successfully!\n","Model artifacts saved in 'model_artifacts' directory\n"]}]}]}